{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/workspace/test.ipynb Cell 1\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f56515f4175676d656e746174696f6e5f227d@ssh-remote%2B166.104.16.55/workspace/test.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f56515f4175676d656e746174696f6e5f227d@ssh-remote%2B166.104.16.55/workspace/test.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# roberta_tokenizer = RobertaTokenizer.from_pretrained(\"roberta-large\")\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f56515f4175676d656e746174696f6e5f227d@ssh-remote%2B166.104.16.55/workspace/test.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m bert_tokenizer \u001b[39m=\u001b[39m BertTokenizer\u001b[39m.\u001b[39;49mfrom_pretrained(\u001b[39m\"\u001b[39;49m\u001b[39mbert-large-uncased\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f56515f4175676d656e746174696f6e5f227d@ssh-remote%2B166.104.16.55/workspace/test.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m bert_model \u001b[39m=\u001b[39m BertForSequenceClassification\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39mbert-large-uncased\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1813\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1811\u001b[0m             resolved_vocab_files[file_id] \u001b[39m=\u001b[39m download_url(file_path, proxies\u001b[39m=\u001b[39mproxies)\n\u001b[1;32m   1812\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1813\u001b[0m         resolved_vocab_files[file_id] \u001b[39m=\u001b[39m cached_file(\n\u001b[1;32m   1814\u001b[0m             pretrained_model_name_or_path,\n\u001b[1;32m   1815\u001b[0m             file_path,\n\u001b[1;32m   1816\u001b[0m             cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m   1817\u001b[0m             force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m   1818\u001b[0m             proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m   1819\u001b[0m             resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[1;32m   1820\u001b[0m             local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m   1821\u001b[0m             token\u001b[39m=\u001b[39;49mtoken,\n\u001b[1;32m   1822\u001b[0m             user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[1;32m   1823\u001b[0m             revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m   1824\u001b[0m             subfolder\u001b[39m=\u001b[39;49msubfolder,\n\u001b[1;32m   1825\u001b[0m             _raise_exceptions_for_missing_entries\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m   1826\u001b[0m             _raise_exceptions_for_connection_errors\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m   1827\u001b[0m             _commit_hash\u001b[39m=\u001b[39;49mcommit_hash,\n\u001b[1;32m   1828\u001b[0m         )\n\u001b[1;32m   1829\u001b[0m         commit_hash \u001b[39m=\u001b[39m extract_commit_hash(resolved_vocab_files[file_id], commit_hash)\n\u001b[1;32m   1831\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(unresolved_files) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/utils/hub.py:429\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    426\u001b[0m user_agent \u001b[39m=\u001b[39m http_user_agent(user_agent)\n\u001b[1;32m    427\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    428\u001b[0m     \u001b[39m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 429\u001b[0m     resolved_file \u001b[39m=\u001b[39m hf_hub_download(\n\u001b[1;32m    430\u001b[0m         path_or_repo_id,\n\u001b[1;32m    431\u001b[0m         filename,\n\u001b[1;32m    432\u001b[0m         subfolder\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m \u001b[39mif\u001b[39;49;00m \u001b[39mlen\u001b[39;49m(subfolder) \u001b[39m==\u001b[39;49m \u001b[39m0\u001b[39;49m \u001b[39melse\u001b[39;49;00m subfolder,\n\u001b[1;32m    433\u001b[0m         repo_type\u001b[39m=\u001b[39;49mrepo_type,\n\u001b[1;32m    434\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m    435\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m    436\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[1;32m    437\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m    438\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m    439\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[1;32m    440\u001b[0m         token\u001b[39m=\u001b[39;49mtoken,\n\u001b[1;32m    441\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m    442\u001b[0m     )\n\u001b[1;32m    443\u001b[0m \u001b[39mexcept\u001b[39;00m GatedRepoError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    444\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    445\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYou are trying to access a gated repo.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mMake sure to request access at \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    446\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mhttps://huggingface.co/\u001b[39m\u001b[39m{\u001b[39;00mpath_or_repo_id\u001b[39m}\u001b[39;00m\u001b[39m and pass a token having permission to this repo either \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    447\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mby logging in with `huggingface-cli login` or by passing `token=<your_token>`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    448\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[39mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    116\u001b[0m     kwargs \u001b[39m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, has_token\u001b[39m=\u001b[39mhas_token, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[0;32m--> 118\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1195\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout)\u001b[0m\n\u001b[1;32m   1193\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1194\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1195\u001b[0m         metadata \u001b[39m=\u001b[39m get_hf_file_metadata(\n\u001b[1;32m   1196\u001b[0m             url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m   1197\u001b[0m             token\u001b[39m=\u001b[39;49mtoken,\n\u001b[1;32m   1198\u001b[0m             proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m   1199\u001b[0m             timeout\u001b[39m=\u001b[39;49metag_timeout,\n\u001b[1;32m   1200\u001b[0m         )\n\u001b[1;32m   1201\u001b[0m     \u001b[39mexcept\u001b[39;00m EntryNotFoundError \u001b[39mas\u001b[39;00m http_error:\n\u001b[1;32m   1202\u001b[0m         \u001b[39m# Cache the non-existence of the file and raise\u001b[39;00m\n\u001b[1;32m   1203\u001b[0m         commit_hash \u001b[39m=\u001b[39m http_error\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mheaders\u001b[39m.\u001b[39mget(HUGGINGFACE_HEADER_X_REPO_COMMIT)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[39mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    116\u001b[0m     kwargs \u001b[39m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, has_token\u001b[39m=\u001b[39mhas_token, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[0;32m--> 118\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1532\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout)\u001b[0m\n\u001b[1;32m   1529\u001b[0m headers[\u001b[39m\"\u001b[39m\u001b[39mAccept-Encoding\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39midentity\u001b[39m\u001b[39m\"\u001b[39m  \u001b[39m# prevent any compression => we want to know the real size of the file\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[39m# Retrieve metadata\u001b[39;00m\n\u001b[0;32m-> 1532\u001b[0m r \u001b[39m=\u001b[39m _request_wrapper(\n\u001b[1;32m   1533\u001b[0m     method\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mHEAD\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m   1534\u001b[0m     url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m   1535\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m   1536\u001b[0m     allow_redirects\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m   1537\u001b[0m     follow_relative_redirects\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   1538\u001b[0m     proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m   1539\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m   1540\u001b[0m )\n\u001b[1;32m   1541\u001b[0m hf_raise_for_status(r)\n\u001b[1;32m   1543\u001b[0m \u001b[39m# Return\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:407\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, max_retries, base_wait_time, max_wait_time, timeout, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[39m# 2. Force relative redirection\u001b[39;00m\n\u001b[1;32m    406\u001b[0m \u001b[39mif\u001b[39;00m follow_relative_redirects:\n\u001b[0;32m--> 407\u001b[0m     response \u001b[39m=\u001b[39m _request_wrapper(\n\u001b[1;32m    408\u001b[0m         method\u001b[39m=\u001b[39;49mmethod,\n\u001b[1;32m    409\u001b[0m         url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m    410\u001b[0m         max_retries\u001b[39m=\u001b[39;49mmax_retries,\n\u001b[1;32m    411\u001b[0m         base_wait_time\u001b[39m=\u001b[39;49mbase_wait_time,\n\u001b[1;32m    412\u001b[0m         max_wait_time\u001b[39m=\u001b[39;49mmax_wait_time,\n\u001b[1;32m    413\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    414\u001b[0m         follow_relative_redirects\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    415\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams,\n\u001b[1;32m    416\u001b[0m     )\n\u001b[1;32m    418\u001b[0m     \u001b[39m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[1;32m    419\u001b[0m     \u001b[39m# This is useful in case of a renamed repository.\u001b[39;00m\n\u001b[1;32m    420\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m300\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m response\u001b[39m.\u001b[39mstatus_code \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m399\u001b[39m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:442\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, max_retries, base_wait_time, max_wait_time, timeout, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[39mreturn\u001b[39;00m response\n\u001b[1;32m    441\u001b[0m \u001b[39m# 3. Exponential backoff\u001b[39;00m\n\u001b[0;32m--> 442\u001b[0m \u001b[39mreturn\u001b[39;00m http_backoff(\n\u001b[1;32m    443\u001b[0m     method\u001b[39m=\u001b[39;49mmethod,\n\u001b[1;32m    444\u001b[0m     url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m    445\u001b[0m     max_retries\u001b[39m=\u001b[39;49mmax_retries,\n\u001b[1;32m    446\u001b[0m     base_wait_time\u001b[39m=\u001b[39;49mbase_wait_time,\n\u001b[1;32m    447\u001b[0m     max_wait_time\u001b[39m=\u001b[39;49mmax_wait_time,\n\u001b[1;32m    448\u001b[0m     retry_on_exceptions\u001b[39m=\u001b[39;49m(Timeout, ProxyError),\n\u001b[1;32m    449\u001b[0m     retry_on_status_codes\u001b[39m=\u001b[39;49m(),\n\u001b[1;32m    450\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    451\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams,\n\u001b[1;32m    452\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_http.py:258\u001b[0m, in \u001b[0;36mhttp_backoff\u001b[0;34m(method, url, max_retries, base_wait_time, max_wait_time, retry_on_exceptions, retry_on_status_codes, **kwargs)\u001b[0m\n\u001b[1;32m    255\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mseek(io_obj_initial_pos)\n\u001b[1;32m    257\u001b[0m \u001b[39m# Perform request and return if status_code is not in the retry list.\u001b[39;00m\n\u001b[0;32m--> 258\u001b[0m response \u001b[39m=\u001b[39m session\u001b[39m.\u001b[39;49mrequest(method\u001b[39m=\u001b[39;49mmethod, url\u001b[39m=\u001b[39;49murl, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    259\u001b[0m \u001b[39mif\u001b[39;00m response\u001b[39m.\u001b[39mstatus_code \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m retry_on_status_codes:\n\u001b[1;32m    260\u001b[0m     \u001b[39mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/requests/sessions.py:587\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    582\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[1;32m    583\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m: timeout,\n\u001b[1;32m    584\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    585\u001b[0m }\n\u001b[1;32m    586\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 587\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(prep, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msend_kwargs)\n\u001b[1;32m    589\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/requests/sessions.py:701\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    698\u001b[0m start \u001b[39m=\u001b[39m preferred_clock()\n\u001b[1;32m    700\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39;49msend(request, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    703\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    704\u001b[0m elapsed \u001b[39m=\u001b[39m preferred_clock() \u001b[39m-\u001b[39m start\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_http.py:63\u001b[0m, in \u001b[0;36mUniqueRequestIdAdapter.send\u001b[0;34m(self, request, *args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[39m\"\"\"Catch any RequestException to append request id to the error message for debugging.\"\"\"\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 63\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49msend(request, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     64\u001b[0m \u001b[39mexcept\u001b[39;00m requests\u001b[39m.\u001b[39mRequestException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     65\u001b[0m     request_id \u001b[39m=\u001b[39m request\u001b[39m.\u001b[39mheaders\u001b[39m.\u001b[39mget(X_AMZN_TRACE_ID)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/requests/adapters.py:489\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    487\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    488\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m chunked:\n\u001b[0;32m--> 489\u001b[0m         resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n\u001b[1;32m    490\u001b[0m             method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[1;32m    491\u001b[0m             url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m    492\u001b[0m             body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n\u001b[1;32m    493\u001b[0m             headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    494\u001b[0m             redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    495\u001b[0m             assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    496\u001b[0m             preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    497\u001b[0m             decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    498\u001b[0m             retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[1;32m    499\u001b[0m             timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    500\u001b[0m         )\n\u001b[1;32m    502\u001b[0m     \u001b[39m# Send the request.\u001b[39;00m\n\u001b[1;32m    503\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    504\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(conn, \u001b[39m\"\u001b[39m\u001b[39mproxy_pool\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py:703\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    700\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_proxy(conn)\n\u001b[1;32m    702\u001b[0m \u001b[39m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[1;32m    704\u001b[0m     conn,\n\u001b[1;32m    705\u001b[0m     method,\n\u001b[1;32m    706\u001b[0m     url,\n\u001b[1;32m    707\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[1;32m    708\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[1;32m    709\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    710\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    711\u001b[0m )\n\u001b[1;32m    713\u001b[0m \u001b[39m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    714\u001b[0m \u001b[39m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    715\u001b[0m \u001b[39m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    716\u001b[0m \u001b[39m# mess.\u001b[39;00m\n\u001b[1;32m    717\u001b[0m response_conn \u001b[39m=\u001b[39m conn \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m release_conn \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py:386\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[39m# Trigger any extra validation we need to do.\u001b[39;00m\n\u001b[1;32m    385\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 386\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_conn(conn)\n\u001b[1;32m    387\u001b[0m \u001b[39mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    388\u001b[0m     \u001b[39m# Py2 raises this as a BaseSSLError, Py3 raises it as socket timeout.\u001b[39;00m\n\u001b[1;32m    389\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_raise_timeout(err\u001b[39m=\u001b[39me, url\u001b[39m=\u001b[39murl, timeout_value\u001b[39m=\u001b[39mconn\u001b[39m.\u001b[39mtimeout)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py:1042\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m   1040\u001b[0m \u001b[39m# Force connect early to allow us to validate the connection.\u001b[39;00m\n\u001b[1;32m   1041\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mgetattr\u001b[39m(conn, \u001b[39m\"\u001b[39m\u001b[39msock\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m):  \u001b[39m# AppEngine might not have  `.sock`\u001b[39;00m\n\u001b[0;32m-> 1042\u001b[0m     conn\u001b[39m.\u001b[39;49mconnect()\n\u001b[1;32m   1044\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m conn\u001b[39m.\u001b[39mis_verified:\n\u001b[1;32m   1045\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m   1046\u001b[0m         (\n\u001b[1;32m   1047\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mUnverified HTTPS request is being made to host \u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1052\u001b[0m         InsecureRequestWarning,\n\u001b[1;32m   1053\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/urllib3/connection.py:358\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    356\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconnect\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    357\u001b[0m     \u001b[39m# Add certificate verification\u001b[39;00m\n\u001b[0;32m--> 358\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msock \u001b[39m=\u001b[39m conn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_new_conn()\n\u001b[1;32m    359\u001b[0m     hostname \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhost\n\u001b[1;32m    360\u001b[0m     tls_in_tls \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/urllib3/connection.py:174\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    171\u001b[0m     extra_kw[\u001b[39m\"\u001b[39m\u001b[39msocket_options\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msocket_options\n\u001b[1;32m    173\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 174\u001b[0m     conn \u001b[39m=\u001b[39m connection\u001b[39m.\u001b[39;49mcreate_connection(\n\u001b[1;32m    175\u001b[0m         (\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dns_host, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mport), \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtimeout, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mextra_kw\n\u001b[1;32m    176\u001b[0m     )\n\u001b[1;32m    178\u001b[0m \u001b[39mexcept\u001b[39;00m SocketTimeout:\n\u001b[1;32m    179\u001b[0m     \u001b[39mraise\u001b[39;00m ConnectTimeoutError(\n\u001b[1;32m    180\u001b[0m         \u001b[39mself\u001b[39m,\n\u001b[1;32m    181\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mConnection to \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m timed out. (connect timeout=\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    182\u001b[0m         \u001b[39m%\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhost, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtimeout),\n\u001b[1;32m    183\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/urllib3/util/connection.py:72\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mUnicodeError\u001b[39;00m:\n\u001b[1;32m     68\u001b[0m     \u001b[39mreturn\u001b[39;00m six\u001b[39m.\u001b[39mraise_from(\n\u001b[1;32m     69\u001b[0m         LocationParseError(\u001b[39mu\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, label empty or too long\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m host), \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     70\u001b[0m     )\n\u001b[0;32m---> 72\u001b[0m \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m socket\u001b[39m.\u001b[39;49mgetaddrinfo(host, port, family, socket\u001b[39m.\u001b[39;49mSOCK_STREAM):\n\u001b[1;32m     73\u001b[0m     af, socktype, proto, canonname, sa \u001b[39m=\u001b[39m res\n\u001b[1;32m     74\u001b[0m     sock \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/socket.py:955\u001b[0m, in \u001b[0;36mgetaddrinfo\u001b[0;34m(host, port, family, type, proto, flags)\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[39m# We override this function since we want to translate the numeric family\u001b[39;00m\n\u001b[1;32m    953\u001b[0m \u001b[39m# and socket type values to enum constants.\u001b[39;00m\n\u001b[1;32m    954\u001b[0m addrlist \u001b[39m=\u001b[39m []\n\u001b[0;32m--> 955\u001b[0m \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m _socket\u001b[39m.\u001b[39;49mgetaddrinfo(host, port, family, \u001b[39mtype\u001b[39;49m, proto, flags):\n\u001b[1;32m    956\u001b[0m     af, socktype, proto, canonname, sa \u001b[39m=\u001b[39m res\n\u001b[1;32m    957\u001b[0m     addrlist\u001b[39m.\u001b[39mappend((_intenum_converter(af, AddressFamily),\n\u001b[1;32m    958\u001b[0m                      _intenum_converter(socktype, SocketKind),\n\u001b[1;32m    959\u001b[0m                      proto, canonname, sa))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# vector to id\n",
    "# id to word\n",
    "\n",
    "from transformers import RobertaTokenizer, BertTokenizer, BertForSequenceClassification  #, RobertaForMaskedLM\n",
    "import torch\n",
    "\n",
    "\n",
    "# roberta_tokenizer = RobertaTokenizer.from_pretrained(\"roberta-large\")\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-large-uncased\")\n",
    "bert_model = BertForSequenceClassification.from_pretrained(\"bert-large-uncased\")\n",
    "# model = RobertaForMaskedLM.from_pretrained(\"roberta-large\")\n",
    "\n",
    "# input_text = \"Hello, how are [MASK] today?\"\n",
    "# inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     outputs = model(**inputs)\n",
    "    \n",
    "# predictions = outputs.logits\n",
    "# predicted_index = torch.argmax(predictions[0, inputs[\"input_ids\"][0] == tokenizer.mask_token_id]).item()\n",
    "# predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])[0]\n",
    "\n",
    "# print(f\"Masked word: {predicted_token}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# with open('/workspace/model_file/bert-large_tokenizer.pkl', \"wb\") as f:\n",
    "#     pickle.dump(bert_tokenizer,f)\n",
    "\n",
    "# bert_model_2 = BertForSequenceClassification.from_pretrained(\"bert-large-uncased\", num_labels =2)\n",
    "# with open('/workspace/model_file/bert-large_class_2.pkl', \"wb\") as f:\n",
    "#     pickle.dump(bert_model_2,f)\n",
    "\n",
    "\n",
    "# bert_model_3 = BertForSequenceClassification.from_pretrained(\"bert-large-uncased\", num_labels =3)\n",
    "# with open('/workspace/model_file/bert-large_class_3.pkl', \"wb\") as f:\n",
    "#     pickle.dump(bert_model_3,f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert_word_embeddings = bert_model_2.bert.embeddings.word_embeddings\n",
    "# with open('/workspace/dataset/bert-large_embeddings.pkl', \"wb\") as f:\n",
    "#     pickle.dump(bert_word_embeddings,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_embeddings_list = []\n",
    "for i in range(30522):\n",
    "    bert_embeddings_list.append(bert_word_embeddings(torch.tensor(i)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(bert_embeddings_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(bert_embeddings_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(bert_embeddings_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_embeddings_list[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/workspace/dataset/bert-large_embeddings.pkl','rb') as f:\n",
    "    temp = pickle.load(f)\n",
    "temp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_embedding_dataset = torch.cat(bert_embeddings_list, dim=0).reshape(len(bert_embeddings_list), 1024)\n",
    "with open('/workspace/dataset/bert_large_embeddings.pkl', 'wb') as f:\n",
    "    pickle.dump(bert_embedding_dataset, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_word_embeddings(torch.tensor(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/workspace/dataset/roberta_large_embeddings.pkl', \"rb\") as f:\n",
    "    roberta_embeddings = pickle.load(f)\n",
    "\n",
    "roberta_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roberta_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_labels = 2)\n",
    "# with open('/workspace/model_file/roberta-large_class_2.pkl', \"wb\") as f:\n",
    "#     pickle.dump(model,f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = BertForSequenceClassification.from_pretrained(\"bert-large-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer.sep_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/workspace/test.ipynb Cell 18\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f56515f4175676d656e746174696f6e5f227d@ssh-remote%2B166.104.16.55/workspace/test.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m text_dataset \u001b[39m=\u001b[39m {}\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f56515f4175676d656e746174696f6e5f227d@ssh-remote%2B166.104.16.55/workspace/test.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mfor\u001b[39;00m data \u001b[39min\u001b[39;00m [\u001b[39m'\u001b[39m\u001b[39msst2\u001b[39m\u001b[39m'\u001b[39m]:  \u001b[39m# 'sst2', 'cola', 'mrpc', 'ax', \u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f56515f4175676d656e746174696f6e5f227d@ssh-remote%2B166.104.16.55/workspace/test.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     dataset \u001b[39m=\u001b[39m load_dataset(\u001b[39m\"\u001b[39;49m\u001b[39mglue\u001b[39;49m\u001b[39m\"\u001b[39;49m, data)\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f56515f4175676d656e746174696f6e5f227d@ssh-remote%2B166.104.16.55/workspace/test.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     text_dataset[data] \u001b[39m=\u001b[39m dataset\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/load.py:2129\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, token, use_auth_token, task, streaming, num_proc, storage_options, **config_kwargs)\u001b[0m\n\u001b[1;32m   2124\u001b[0m verification_mode \u001b[39m=\u001b[39m VerificationMode(\n\u001b[1;32m   2125\u001b[0m     (verification_mode \u001b[39mor\u001b[39;00m VerificationMode\u001b[39m.\u001b[39mBASIC_CHECKS) \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m save_infos \u001b[39melse\u001b[39;00m VerificationMode\u001b[39m.\u001b[39mALL_CHECKS\n\u001b[1;32m   2126\u001b[0m )\n\u001b[1;32m   2128\u001b[0m \u001b[39m# Create a dataset builder\u001b[39;00m\n\u001b[0;32m-> 2129\u001b[0m builder_instance \u001b[39m=\u001b[39m load_dataset_builder(\n\u001b[1;32m   2130\u001b[0m     path\u001b[39m=\u001b[39;49mpath,\n\u001b[1;32m   2131\u001b[0m     name\u001b[39m=\u001b[39;49mname,\n\u001b[1;32m   2132\u001b[0m     data_dir\u001b[39m=\u001b[39;49mdata_dir,\n\u001b[1;32m   2133\u001b[0m     data_files\u001b[39m=\u001b[39;49mdata_files,\n\u001b[1;32m   2134\u001b[0m     cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m   2135\u001b[0m     features\u001b[39m=\u001b[39;49mfeatures,\n\u001b[1;32m   2136\u001b[0m     download_config\u001b[39m=\u001b[39;49mdownload_config,\n\u001b[1;32m   2137\u001b[0m     download_mode\u001b[39m=\u001b[39;49mdownload_mode,\n\u001b[1;32m   2138\u001b[0m     revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m   2139\u001b[0m     token\u001b[39m=\u001b[39;49mtoken,\n\u001b[1;32m   2140\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[1;32m   2141\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mconfig_kwargs,\n\u001b[1;32m   2142\u001b[0m )\n\u001b[1;32m   2144\u001b[0m \u001b[39m# Return iterable dataset in case of streaming\u001b[39;00m\n\u001b[1;32m   2145\u001b[0m \u001b[39mif\u001b[39;00m streaming:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/load.py:1815\u001b[0m, in \u001b[0;36mload_dataset_builder\u001b[0;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, use_auth_token, storage_options, **config_kwargs)\u001b[0m\n\u001b[1;32m   1813\u001b[0m     download_config \u001b[39m=\u001b[39m download_config\u001b[39m.\u001b[39mcopy() \u001b[39mif\u001b[39;00m download_config \u001b[39melse\u001b[39;00m DownloadConfig()\n\u001b[1;32m   1814\u001b[0m     download_config\u001b[39m.\u001b[39mstorage_options\u001b[39m.\u001b[39mupdate(storage_options)\n\u001b[0;32m-> 1815\u001b[0m dataset_module \u001b[39m=\u001b[39m dataset_module_factory(\n\u001b[1;32m   1816\u001b[0m     path,\n\u001b[1;32m   1817\u001b[0m     revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m   1818\u001b[0m     download_config\u001b[39m=\u001b[39;49mdownload_config,\n\u001b[1;32m   1819\u001b[0m     download_mode\u001b[39m=\u001b[39;49mdownload_mode,\n\u001b[1;32m   1820\u001b[0m     data_dir\u001b[39m=\u001b[39;49mdata_dir,\n\u001b[1;32m   1821\u001b[0m     data_files\u001b[39m=\u001b[39;49mdata_files,\n\u001b[1;32m   1822\u001b[0m )\n\u001b[1;32m   1823\u001b[0m \u001b[39m# Get dataset builder class from the processing script\u001b[39;00m\n\u001b[1;32m   1824\u001b[0m builder_kwargs \u001b[39m=\u001b[39m dataset_module\u001b[39m.\u001b[39mbuilder_kwargs\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/load.py:1453\u001b[0m, in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, **download_kwargs)\u001b[0m\n\u001b[1;32m   1451\u001b[0m hf_api \u001b[39m=\u001b[39m HfApi(config\u001b[39m.\u001b[39mHF_ENDPOINT)\n\u001b[1;32m   1452\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1453\u001b[0m     dataset_info \u001b[39m=\u001b[39m hf_api\u001b[39m.\u001b[39;49mdataset_info(\n\u001b[1;32m   1454\u001b[0m         repo_id\u001b[39m=\u001b[39;49mpath,\n\u001b[1;32m   1455\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m   1456\u001b[0m         token\u001b[39m=\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49mtoken,\n\u001b[1;32m   1457\u001b[0m         timeout\u001b[39m=\u001b[39;49m\u001b[39m100.0\u001b[39;49m,\n\u001b[1;32m   1458\u001b[0m     )\n\u001b[1;32m   1459\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# noqa catch any exception of hf_hub and consider that the dataset doesn't exist\u001b[39;00m\n\u001b[1;32m   1460\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\n\u001b[1;32m   1461\u001b[0m         e,\n\u001b[1;32m   1462\u001b[0m         (\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1466\u001b[0m         ),\n\u001b[1;32m   1467\u001b[0m     ):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[39mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    116\u001b[0m     kwargs \u001b[39m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, has_token\u001b[39m=\u001b[39mhas_token, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[0;32m--> 118\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/huggingface_hub/hf_api.py:1740\u001b[0m, in \u001b[0;36mHfApi.dataset_info\u001b[0;34m(self, repo_id, revision, timeout, files_metadata, token)\u001b[0m\n\u001b[1;32m   1737\u001b[0m \u001b[39mif\u001b[39;00m files_metadata:\n\u001b[1;32m   1738\u001b[0m     params[\u001b[39m\"\u001b[39m\u001b[39mblobs\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m-> 1740\u001b[0m r \u001b[39m=\u001b[39m get_session()\u001b[39m.\u001b[39;49mget(path, headers\u001b[39m=\u001b[39;49mheaders, timeout\u001b[39m=\u001b[39;49mtimeout, params\u001b[39m=\u001b[39;49mparams)\n\u001b[1;32m   1741\u001b[0m hf_raise_for_status(r)\n\u001b[1;32m   1742\u001b[0m d \u001b[39m=\u001b[39m r\u001b[39m.\u001b[39mjson()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/requests/sessions.py:600\u001b[0m, in \u001b[0;36mSession.get\u001b[0;34m(self, url, **kwargs)\u001b[0m\n\u001b[1;32m    592\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"Sends a GET request. Returns :class:`Response` object.\u001b[39;00m\n\u001b[1;32m    593\u001b[0m \n\u001b[1;32m    594\u001b[0m \u001b[39m:param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m    595\u001b[0m \u001b[39m:param \\*\\*kwargs: Optional arguments that ``request`` takes.\u001b[39;00m\n\u001b[1;32m    596\u001b[0m \u001b[39m:rtype: requests.Response\u001b[39;00m\n\u001b[1;32m    597\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    599\u001b[0m kwargs\u001b[39m.\u001b[39msetdefault(\u001b[39m\"\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m--> 600\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrequest(\u001b[39m\"\u001b[39;49m\u001b[39mGET\u001b[39;49m\u001b[39m\"\u001b[39;49m, url, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/requests/sessions.py:587\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    582\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[1;32m    583\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m: timeout,\n\u001b[1;32m    584\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    585\u001b[0m }\n\u001b[1;32m    586\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 587\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(prep, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msend_kwargs)\n\u001b[1;32m    589\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/requests/sessions.py:701\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    698\u001b[0m start \u001b[39m=\u001b[39m preferred_clock()\n\u001b[1;32m    700\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39;49msend(request, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    703\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    704\u001b[0m elapsed \u001b[39m=\u001b[39m preferred_clock() \u001b[39m-\u001b[39m start\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_http.py:63\u001b[0m, in \u001b[0;36mUniqueRequestIdAdapter.send\u001b[0;34m(self, request, *args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[39m\"\"\"Catch any RequestException to append request id to the error message for debugging.\"\"\"\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 63\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49msend(request, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     64\u001b[0m \u001b[39mexcept\u001b[39;00m requests\u001b[39m.\u001b[39mRequestException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     65\u001b[0m     request_id \u001b[39m=\u001b[39m request\u001b[39m.\u001b[39mheaders\u001b[39m.\u001b[39mget(X_AMZN_TRACE_ID)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/requests/adapters.py:489\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    487\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    488\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m chunked:\n\u001b[0;32m--> 489\u001b[0m         resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n\u001b[1;32m    490\u001b[0m             method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[1;32m    491\u001b[0m             url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m    492\u001b[0m             body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n\u001b[1;32m    493\u001b[0m             headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    494\u001b[0m             redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    495\u001b[0m             assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    496\u001b[0m             preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    497\u001b[0m             decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    498\u001b[0m             retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[1;32m    499\u001b[0m             timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    500\u001b[0m         )\n\u001b[1;32m    502\u001b[0m     \u001b[39m# Send the request.\u001b[39;00m\n\u001b[1;32m    503\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    504\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(conn, \u001b[39m\"\u001b[39m\u001b[39mproxy_pool\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py:703\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    700\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_proxy(conn)\n\u001b[1;32m    702\u001b[0m \u001b[39m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[1;32m    704\u001b[0m     conn,\n\u001b[1;32m    705\u001b[0m     method,\n\u001b[1;32m    706\u001b[0m     url,\n\u001b[1;32m    707\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[1;32m    708\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[1;32m    709\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    710\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    711\u001b[0m )\n\u001b[1;32m    713\u001b[0m \u001b[39m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    714\u001b[0m \u001b[39m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    715\u001b[0m \u001b[39m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    716\u001b[0m \u001b[39m# mess.\u001b[39;00m\n\u001b[1;32m    717\u001b[0m response_conn \u001b[39m=\u001b[39m conn \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m release_conn \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py:386\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[39m# Trigger any extra validation we need to do.\u001b[39;00m\n\u001b[1;32m    385\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 386\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_conn(conn)\n\u001b[1;32m    387\u001b[0m \u001b[39mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    388\u001b[0m     \u001b[39m# Py2 raises this as a BaseSSLError, Py3 raises it as socket timeout.\u001b[39;00m\n\u001b[1;32m    389\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_raise_timeout(err\u001b[39m=\u001b[39me, url\u001b[39m=\u001b[39murl, timeout_value\u001b[39m=\u001b[39mconn\u001b[39m.\u001b[39mtimeout)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py:1042\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m   1040\u001b[0m \u001b[39m# Force connect early to allow us to validate the connection.\u001b[39;00m\n\u001b[1;32m   1041\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mgetattr\u001b[39m(conn, \u001b[39m\"\u001b[39m\u001b[39msock\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m):  \u001b[39m# AppEngine might not have  `.sock`\u001b[39;00m\n\u001b[0;32m-> 1042\u001b[0m     conn\u001b[39m.\u001b[39;49mconnect()\n\u001b[1;32m   1044\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m conn\u001b[39m.\u001b[39mis_verified:\n\u001b[1;32m   1045\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m   1046\u001b[0m         (\n\u001b[1;32m   1047\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mUnverified HTTPS request is being made to host \u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1052\u001b[0m         InsecureRequestWarning,\n\u001b[1;32m   1053\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/urllib3/connection.py:358\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    356\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconnect\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    357\u001b[0m     \u001b[39m# Add certificate verification\u001b[39;00m\n\u001b[0;32m--> 358\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msock \u001b[39m=\u001b[39m conn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_new_conn()\n\u001b[1;32m    359\u001b[0m     hostname \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhost\n\u001b[1;32m    360\u001b[0m     tls_in_tls \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/urllib3/connection.py:174\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    171\u001b[0m     extra_kw[\u001b[39m\"\u001b[39m\u001b[39msocket_options\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msocket_options\n\u001b[1;32m    173\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 174\u001b[0m     conn \u001b[39m=\u001b[39m connection\u001b[39m.\u001b[39;49mcreate_connection(\n\u001b[1;32m    175\u001b[0m         (\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dns_host, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mport), \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtimeout, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mextra_kw\n\u001b[1;32m    176\u001b[0m     )\n\u001b[1;32m    178\u001b[0m \u001b[39mexcept\u001b[39;00m SocketTimeout:\n\u001b[1;32m    179\u001b[0m     \u001b[39mraise\u001b[39;00m ConnectTimeoutError(\n\u001b[1;32m    180\u001b[0m         \u001b[39mself\u001b[39m,\n\u001b[1;32m    181\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mConnection to \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m timed out. (connect timeout=\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    182\u001b[0m         \u001b[39m%\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhost, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtimeout),\n\u001b[1;32m    183\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/urllib3/util/connection.py:72\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mUnicodeError\u001b[39;00m:\n\u001b[1;32m     68\u001b[0m     \u001b[39mreturn\u001b[39;00m six\u001b[39m.\u001b[39mraise_from(\n\u001b[1;32m     69\u001b[0m         LocationParseError(\u001b[39mu\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, label empty or too long\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m host), \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     70\u001b[0m     )\n\u001b[0;32m---> 72\u001b[0m \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m socket\u001b[39m.\u001b[39;49mgetaddrinfo(host, port, family, socket\u001b[39m.\u001b[39;49mSOCK_STREAM):\n\u001b[1;32m     73\u001b[0m     af, socktype, proto, canonname, sa \u001b[39m=\u001b[39m res\n\u001b[1;32m     74\u001b[0m     sock \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/socket.py:955\u001b[0m, in \u001b[0;36mgetaddrinfo\u001b[0;34m(host, port, family, type, proto, flags)\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[39m# We override this function since we want to translate the numeric family\u001b[39;00m\n\u001b[1;32m    953\u001b[0m \u001b[39m# and socket type values to enum constants.\u001b[39;00m\n\u001b[1;32m    954\u001b[0m addrlist \u001b[39m=\u001b[39m []\n\u001b[0;32m--> 955\u001b[0m \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m _socket\u001b[39m.\u001b[39;49mgetaddrinfo(host, port, family, \u001b[39mtype\u001b[39;49m, proto, flags):\n\u001b[1;32m    956\u001b[0m     af, socktype, proto, canonname, sa \u001b[39m=\u001b[39m res\n\u001b[1;32m    957\u001b[0m     addrlist\u001b[39m.\u001b[39mappend((_intenum_converter(af, AddressFamily),\n\u001b[1;32m    958\u001b[0m                      _intenum_converter(socktype, SocketKind),\n\u001b[1;32m    959\u001b[0m                      proto, canonname, sa))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import pickle\n",
    "\n",
    "text_dataset = {}\n",
    "\n",
    "for data in ['sst2']:  # 'sst2', 'cola', 'mrpc', 'ax', \n",
    "    dataset = load_dataset(\"glue\", data)\n",
    "    text_dataset[data] = dataset\n",
    "    \n",
    "    \n",
    "    # with open('/workspace/task_dataset/%s_dataset.pkl'%data, 'wb') as f:\n",
    "    #     pickle.dump(dataset, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['train']['sentence'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name_mapping = {\n",
    "    'cola': 'CoLA',\n",
    "    'sst-2': 'SST-2',\n",
    "    'qqp': 'QQP',\n",
    "    'mrpc': 'MRPC',\n",
    "    'mnli': 'MNLI-m',\n",
    "    'mnli-mm': 'MNLI-mm',\n",
    "    'qnli': 'QNLI',\n",
    "    'rte': 'RTE',\n",
    "    'wnli': 'WNLI',\n",
    "    'stsb' : 'STS-B',\n",
    "    'ax' : 'AX'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# save_conventional_test_result = f'/workspace/test_convention_result'\n",
    "\n",
    "# location = f'{save_conventional_test_result}/model_roberta-large_lr_1e-05_seed_13'\n",
    "\n",
    "# datanames = ['cola', 'sst-2', 'qqp', 'mrpc', 'mnli', 'mnli-mm', 'qnli', 'rte', 'wnli', 'stsb', 'ax']\n",
    "# for dataname in datanames:\n",
    "#     for location in [f'{save_conventional_test_result}/model_roberta-large_lr_1e-05_seed_13','/workspace/sample']:\n",
    "#         NAME = dataset_name_mapping[dataname]\n",
    "#         data = pd.read_csv(location + f'/{NAME}.tsv', sep='\\t')\n",
    "        \n",
    "#         print(NAME,set(data['prediction']))\n",
    "#         print(data.shape)\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_conventional_test_result = '/workspace/test_convention_result'\n",
    "# location = f'{save_conventional_test_result}/model_roberta-large_lr_1e-05_seed_13'\n",
    "# NAME = dataset_name_mapping[dataname]\n",
    "# data = pd.read_csv(location + f'/{NAME}.tsv', sep='\\t')\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# location = '/workspace/sample'\n",
    "# NAME = dataset_name_mapping[dataname]\n",
    "# data = pd.read_csv(location + f'/{NAME}.tsv', sep='\\t')\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('workspace/model_file/roberta-large_class_2.pkl', 'rb') as f:\n",
    "    roberta_model = pickle.load(f)\n",
    "\n",
    "roberta_model.get_input_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50265, 1024])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('workspace/dataset/roberta-large_embeddings.pkl', 'rb') as f:\n",
    "    model_embeddings = pickle.load(f)\n",
    "model_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "model_path = '/workspace/model_file/roberta-large_class_3.pkl'\n",
    "with open(model_path, \"rb\") as f:\n",
    "    model1 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CoLA {1}\n",
      "(1063, 2)\n",
      "SST-2 {0, 1}\n",
      "(1821, 2)\n",
      "QQP {0, 1}\n",
      "(390965, 2)\n",
      "MRPC {0, 1}\n",
      "(1725, 2)\n",
      "MNLI-m {'contradiction', 'neutral', 'entailment'}\n",
      "(9796, 2)\n",
      "MNLI-mm {'contradiction', 'entailment', 'neutral'}\n",
      "(9847, 2)\n",
      "QNLI {'not_entailment', 'entailment'}\n",
      "(5463, 2)\n",
      "RTE {'not_entailment', 'entailment'}\n",
      "(3000, 2)\n",
      "WNLI {0, 1}\n",
      "(146, 2)\n",
      "STS-B {1.638, 2.75, 3.707, 3.418, 3.417, 3.165, 2.375, 2.933, 2.878, 2.683, 3.192, 3.872, 4.305, 2.875, 3.0, 3.125, 3.25, 3.5, 3.375, 1.93, 1.491, 2.82, 2.991, 2.741, 2.61, 2.11, 2.866, 3.36, 3.866, 3.485, 3.991, 3.235, 3.116, 3.241, 3.366, 1.601, 1.851, 2.68, 2.93, 2.976, 2.107, 2.851, 2.482, 2.607, 3.357, 3.851, 3.351, 2.554, 2.929, 3.232, 3.607, 2.804, 3.482, 3.226, 2.211, 2.217, 2.336, 2.717, 2.086, 3.586, 3.711, 2.836, 3.842, 3.461, 3.467, 3.342, 3.086, 2.538, 1.952, 2.458, 2.958, 2.208, 2.952, 2.452, 3.333, 3.202, 3.833, 2.833, 3.952, 4.071, 4.196, 3.577, 2.693, 2.937, 2.568, 2.068, 2.812, 3.443, 3.187, 2.818, 3.437, 2.062, 3.687, 3.062, 3.562, 3.193, 3.318, 2.178, 2.428, 2.684, 2.928, 2.303, 3.178, 3.553, 3.934, 3.803, 3.434, 4.178, 1.989, 4.059, 3.684, 3.303, 2.294, 2.663, 2.913, 2.794, 2.919, 3.169, 3.669, 3.413, 3.038, 3.419, 3.788, 3.288, 3.919, 3.044, 1.904, 2.66, 2.91, 2.285, 2.154, 2.535, 3.41, 3.654, 3.035, 3.404, 3.785, 4.029, 3.279, 3.535, 2.545, 3.029, 1.215, 1.59, 4.535, 2.645, 2.395, 2.514, 2.77, 2.895, 3.52, 3.02, 3.514, 3.395, 3.014, 1.95, 2.029, 2.255, 2.63, 2.261, 2.755, 2.886, 3.005, 3.38, 3.13, 3.136, 3.636, 4.124, 3.386, 3.761, 4.142, 3.154, 1.191, 1.496, 2.871, 2.996, 2.263, 3.746, 3.496, 3.121, 3.246, 3.871, 3.371, 2.443, 1.987, 1.862, 2.987, 2.862, 2.487, 2.231, 2.731, 3.112, 2.606, 2.481, 2.612, 3.356, 4.106, 4.237, 3.737, 2.872, 2.747, 1.597, 1.972, 1.847, 2.927, 2.427, 2.972, 2.478, 2.472, 2.978, 2.722, 3.472, 3.853, 3.097, 3.353, 3.347, 3.222, 3.103, 1.777, 2.981, 1.838, 1.963, 2.838, 2.832, 2.963, 2.457, 2.088, 3.332, 3.582, 3.457, 3.838, 3.082, 3.338, 2.123, 2.841, 2.466, 2.454, 2.448, 2.823, 2.704, 2.954, 3.204, 3.198, 3.454, 3.329, 3.323, 3.079, 2.465, 3.448, 3.948, 2.09, 1.747, 1.305, 1.845, 1.595, 1.675, 1.736, 1.962, 1.816, 1.981, 1.939, 2.308, 2.939, 2.189, 2.183, 3.064, 3.308, 3.564, 3.189, 3.058, 3.314, 1.744, 1.869, 4.183, 2.449, 1.799, 2.18, 2.549, 2.305, 2.924, 2.555, 3.055, 3.549, 3.805, 3.555, 3.049, 3.674, 2.159, 2.79, 2.784, 2.534, 3.54, 3.034, 3.534, 3.159, 3.665, 3.29, 3.909, 3.309, 3.04, 3.059, 1.72, 2.113, 2.488, 2.65, 2.906, 2.781, 2.281, 3.275, 3.156, 3.4, 3.281, 3.531, 3.9, 1.961, 3.031, 3.025, 3.406, 2.667, 1.391, 1.516, 2.766, 2.51, 2.885, 2.891, 3.141, 3.135, 3.391, 3.51, 4.016, 4.147, 2.277, 1.571, 2.901, 2.251, 2.257, 2.757, 2.507, 2.632, 3.007, 3.376, 3.501, 3.507, 3.132, 2.706, 2.636, 1.992, 3.956, 2.867, 2.623, 2.617, 2.492, 2.998, 3.498, 3.992, 3.623, 3.123, 3.242, 3.873, 3.373, 3.617, 3.117, 1.797, 2.315, 1.608, 2.358, 2.602, 2.727, 2.995, 2.608, 3.227, 3.358, 3.233, 3.477, 3.108, 2.37, 3.899, 3.977, 1.718, 1.843, 2.174, 2.968, 2.718, 2.218, 2.349, 2.474, 3.718, 3.599, 3.349, 3.093, 3.468, 4.23, 4.087, 3.218, 1.273, 4.48, 2.104, 2.959, 2.459, 2.828, 2.953, 2.834, 3.084, 3.459, 3.078, 3.334, 3.703, 3.828, 3.209, 3.709, 3.203, 1.819, 1.944, 2.819, 2.944, 2.7, 2.444, 2.569, 3.194, 3.944, 3.325, 3.819, 3.45, 4.2, 3.075, 3.2, 3.569, 2.679, 2.179, 2.304, 2.429, 2.31, 3.179, 3.804, 2.435, 2.685, 2.56, 3.685, 3.304, 3.429, 3.31, 3.554, 2.301, 2.176, 2.295, 2.67, 2.051, 3.301, 3.545, 3.676, 3.295, 2.795, 4.307, 4.045, 3.67, 3.176, 3.795, 1.411, 1.036, 2.905, 2.911, 2.036, 3.28, 3.155, 3.536, 3.53, 3.411, 3.405, 1.591, 3.036, 3.681, 1.716, 1.771, 2.027, 2.152, 2.646, 2.902, 2.777, 3.396, 3.652, 3.902, 3.527, 3.646, 3.521, 3.402, 3.146, 3.771, 3.896, 2.344, 2.256, 2.131, 2.756, 2.262, 2.512, 3.506, 3.006, 3.881, 3.387, 3.012, 3.762, 1.692, 1.988, 2.85, 3.693, 3.695, 3.631, 3.336, 3.063, 3.799, 2.887, 1.674, 2.622, 3.43, 2.503, 2.997, 2.003, 3.997, 3.247, 3.622, 3.378, 3.497, 4.122, 4.009, 3.003, 3.503, 3.372, 2.883, 1.488, 1.863, 2.738, 2.869, 2.369, 2.619, 2.613, 3.613, 3.238, 3.619, 3.869, 3.494, 3.363, 3.369, 3.515, 2.354, 2.723, 2.848, 2.729, 2.742, 3.348, 3.479, 3.598, 2.979, 3.104, 4.092, 3.312, 4.854, 2.797, 2.422, 1.464, 2.97, 2.72, 2.345, 2.589, 3.22, 3.339, 3.714, 3.595, 3.464, 3.345, 3.97, 1.894, 2.726, 2.705, 2.949, 2.699, 2.455, 2.955, 3.324, 3.705, 3.955, 3.199, 3.08, 4.205, 3.455, 3.824, 2.46, 2.821, 2.94, 2.815, 2.946, 2.696, 3.696, 3.565, 3.315, 3.446, 3.44, 3.065, 3.196, 2.765, 1.931, 2.056, 2.431, 2.806, 2.931, 2.181, 3.3, 3.181, 3.8, 3.306, 3.05, 3.425, 1.611, 4.056, 4.044, 3.431, 2.694, 4.3, 1.861, 2.672, 2.922, 2.547, 2.666, 2.916, 3.416, 3.422, 3.797, 3.166, 3.047, 3.172, 3.672, 3.297, 3.547, 3.291, 1.602, 1.782, 2.651, 2.782, 2.907, 2.657, 2.532, 3.776, 3.282, 3.651, 3.276, 3.526, 4.413, 3.401, 3.532, 4.02, 4.395, 4.52, 4.782, 2.267, 2.517, 2.017, 2.773, 3.398, 3.898, 3.517, 3.767, 3.148, 3.273, 3.648, 2.752, 2.258, 2.877, 2.627, 2.252, 3.383, 3.127, 3.377, 3.133, 3.877, 4.014, 3.008, 3.508, 2.521, 3.752, 4.008, 1.749, 2.201, 2.374, 2.999, 2.249, 2.368, 2.868, 3.124, 2.118, 2.624, 3.374, 3.624, 3.749, 3.249, 3.743, 3.243, 1.804, 1.734, 2.615, 2.609, 2.99, 2.359, 2.74, 3.609, 3.109, 3.24, 2.935, 2.484, 4.228, 3.984, 2.725, 2.219, 2.094, 2.614, 2.475, 3.1, 3.475, 3.969, 2.975, 3.219, 3.6, 3.344, 3.725, 3.469, 2.544, 1.835, 3.489, 3.739, 2.591, 2.966, 2.585, 2.71, 2.716, 3.466, 3.21, 3.71, 2.216, 3.085, 4.079, 4.091, 2.849, 3.46, 3.335, 2.334, 1.826, 1.451, 3.224, 2.701, 2.945, 2.07, 2.826, 2.903, 3.326, 3.32, 3.82, 3.201, 3.451, 1.756, 1.131, 2.242, 1.482, 1.796, 1.873, 1.309, 1.599, 1.913, 1.7, 1.389, 1.926, 1.551, 2.067, 2.561, 2.936, 2.311, 2.942, 3.561, 3.942, 3.317, 3.811, 3.436, 3.567, 1.872, 3.186, 2.567, 2.302, 2.552, 2.677, 2.921, 2.171, 3.046, 3.427, 3.052, 3.802, 2.941, 3.421, 3.302, 3.171, 3.796, 1.912, 2.787, 2.668, 2.918, 2.418, 2.037, 3.043, 3.162, 3.537, 3.168, 3.293, 3.668, 3.787, 4.406, 2.175, 2.397, 2.278, 2.403, 2.522, 2.778, 3.272, 3.278, 3.028, 2.653, 3.403, 3.528, 3.153, 3.022, 3.772, 3.647, 1.958, 1.888, 2.144, 2.519, 2.638, 2.769, 2.394, 3.769, 2.269, 3.013, 2.513, 2.888, 3.019, 4.519, 3.269, 3.519, 3.285, 3.409, 2.754, 2.644, 3.129, 3.254, 3.379, 3.504, 3.629, 3.754, 1.934, 1.809, 1.495, 2.573, 2.698, 2.948, 2.12, 2.489, 2.87, 2.989, 2.739, 3.114, 2.864, 3.364, 3.239, 2.62, 3.995, 4.126, 4.001, 3.614, 3.62, 2.378, 2.128, 4.114, 1.3, 1.73, 1.855, 2.861, 2.605, 3.105, 3.361, 3.111, 3.23, 3.73, 3.605, 3.48, 3.486, 3.611, 2.432, 2.807, 2.682, 2.362, 1.846, 3.768, 2.965, 2.721, 2.096, 2.346, 2.596, 3.215, 3.09, 3.715, 2.59, 2.846, 3.221, 4.084, 3.84, 3.465, 4.096, 3.237, 3.862, 2.541, 1.581, 2.956, 2.962, 2.462, 2.587, 2.087, 3.081, 3.206, 3.706, 3.581, 3.087, 3.337, 2.471, 3.331, 3.041, 3.541, 1.886, 2.483, 2.108, 2.947, 2.691, 2.322, 2.822, 2.697, 3.322, 3.691, 3.316, 3.566, 3.197, 3.447, 1.752, 1.932, 1.432, 2.688, 2.932, 2.307, 2.438, 2.182, 3.563, 3.557, 3.313, 3.057, 3.182, 3.188, 2.884, 2.134, 2.759, 2.384, 2.332, 2.298, 2.173, 2.792, 2.798, 2.423, 3.292, 3.542, 3.423, 3.673, 3.173, 3.042, 3.759, 1.478, 2.119, 2.244, 1.811, 2.993, 1.783, 2.789, 2.914, 2.908, 2.533, 2.658, 3.414, 3.283, 2.783, 3.289, 3.664, 3.908, 1.594, 1.969, 3.033, 3.158, 2.228, 2.393, 2.774, 2.268, 2.399, 2.852, 3.024, 3.149, 3.274, 3.518, 3.774, 4.012, 1.954, 3.399, 3.018, 3.649, 1.509, 2.64, 2.015, 2.265, 2.009, 2.515, 3.509, 3.64, 2.39, 2.89, 3.015, 3.134, 1.82, 3.259, 3.384, 2.712}\n",
      "(1379, 2)\n",
      "AX {'entailment', 'neutral', 'contradiction'}\n",
      "(1104, 2)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "save_conventional_test_result = f'/workspace/sample'\n",
    "\n",
    "location = f'{save_conventional_test_result}'\n",
    "datanames = ['cola', 'sst-2', 'qqp', 'mrpc', 'mnli', 'mnli-mm', 'qnli', 'rte', 'wnli', 'stsb', 'ax']\n",
    "for dataname in datanames:\n",
    "    NAME = dataset_name_mapping[dataname]\n",
    "    data = pd.read_csv(location + f'/{NAME}.tsv', sep='\\t')\n",
    "    \n",
    "    print(NAME,set(data['prediction']))\n",
    "    print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>contradiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1099</th>\n",
       "      <td>1099</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1100</th>\n",
       "      <td>1100</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1101</th>\n",
       "      <td>1101</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1102</th>\n",
       "      <td>1102</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1103</th>\n",
       "      <td>1103</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1104 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index     prediction\n",
       "0         0  contradiction\n",
       "1         1     entailment\n",
       "2         2     entailment\n",
       "3         3     entailment\n",
       "4         4     entailment\n",
       "...     ...            ...\n",
       "1099   1099     entailment\n",
       "1100   1100     entailment\n",
       "1101   1101     entailment\n",
       "1102   1102     entailment\n",
       "1103   1103     entailment\n",
       "\n",
       "[1104 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(data['prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import os\n",
    "\n",
    "# lrs= [1e-05, 2e-05, 5e-05]\n",
    "# seeds = [13, 42, 100]\n",
    "# save_conventional_test_result = f'/workspace/test_convention_result'\n",
    "\n",
    "# for lr in lrs:\n",
    "#     for seed in seeds:\n",
    "#         save_test_result_specific = f'{save_conventional_test_result}/model_roberta-large_lr_{lr}_seed_{seed}'\n",
    "#         dataset_names = ['stsb','ax']\n",
    "#         for data_name in dataset_names:\n",
    "#             data = dataset[data_name]['test'] \n",
    "#             with open(os.path.join(save_test_result_specific, \"%s.tsv\" % (dataset_name_mapping[data_name])), 'w') as pred_fh:\n",
    "#                 pred_fh.write(\"index\\tprediction\\n\")\n",
    "#                 for idx, pred in enumerate(range(data.num_rows)):\n",
    "#                     if data_name == 'stsb':\n",
    "#                         pred_fh.write('%d\\t%s\\n' % (idx, 2.5))\n",
    "#                     if data_name == 'ax':\n",
    "#                         pred_fh.write('%d\\t%s\\n' % (idx, 'entailment'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # dummy dataset 만드는 코드\n",
    "\n",
    "# import os\n",
    "# save_conventional_test_result = '/workspace/test_convention_result'\n",
    "# lr = 1e-05\n",
    "# seed = 13\n",
    "\n",
    "# save_test_result_specific = f'{save_conventional_test_result}/model_roberta-large_lr_{lr}_seed_{seed}'\n",
    "# dataset_names = ['stsb','ax']\n",
    "# for data_name in dataset_names:\n",
    "#     data = dataset[data_name]['test'] \n",
    "#     with open(os.path.join(save_test_result_specific, \"%s.tsv\" % (dataset_name_mapping[data_name])), 'w') as pred_fh:\n",
    "#         pred_fh.write(\"index\\tprediction\\n\")\n",
    "#         for idx, pred in enumerate(range(data.num_rows)):\n",
    "#             pred_fh.write('%d\\t%s\\n' % (idx, '0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import RobertaForSequenceClassification, RobertaConfig\n",
    "# import os\n",
    "# import pickle\n",
    "\n",
    "# model = RobertaForSequenceClassification.from_pretrained(\"roberta-large\", num_labels = 2)\n",
    "# with open('/workspace/model_file/roberta-large_class_2.pkl', \"wb\") as f:\n",
    "#     pickle.dump(model,f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # 모델 불러오기\n",
    "\n",
    "# from transformers import RobertaForSequenceClassification  # RobertaTokenizer\n",
    "# import torch\n",
    "\n",
    "# # Load tokenizer and model\n",
    "# # tokenizer = RobertaTokenizer.from_pretrained('roberta-large')\n",
    "# model = RobertaForSequenceClassification.from_pretrained('roberta-large-mnli')\n",
    "\n",
    "# with open('/workspace/model_file/roberta-large-mnli.pkl', 'wb') as f:\n",
    "#     pickle.dump(model, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('/workspace/model_file/roberta-large_class_3.pkl', \"rb\") as f:\n",
    "#     model1 = pickle.load(f)\n",
    "\n",
    "# with open('/workspace/model_file/roberta-large-mnli.pkl', \"rb\") as f:\n",
    "#     model2 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "def sum_text(texts1, texts2, sep_token):\n",
    "    full_text = np.array(texts1,dtype=object) + np.array([sep_token for _ in range(len(texts1))])+np.array(texts2,dtype=object)\n",
    "\n",
    "    return list(full_text)\n",
    "\n",
    "datasets = {}\n",
    "data_names = ['sst-2', 'mrpc',]\n",
    "# data_names = ['qqp', 'cola','sst-2', 'mrpc', 'wnli', 'qnli', 'rte', 'mnli', 'mnli-mm']\n",
    "for data_name in data_names:\n",
    "    location = f'/workspace/task_dataset/{data_name}_dataset.pkl'\n",
    "    with open(location, 'rb') as f:\n",
    "        datasets[data_name] = pickle.load(f)\n",
    "\n",
    "\n",
    "sentences_data = datasets['sst-2']['train'][:20]\n",
    "\n",
    "# line_text_encoding = sum_text(sentences_data['premise'],sentences_data['hypothesis'],tokenizer.sep_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'sst-2'\n",
    "with open(f'/workspace/cache_index/{dataset}_seed_13_indexes.pkl', 'rb') as f:\n",
    "    dataset_index = pickle.load(f)\n",
    "sentences_data = datasets['sst-2']['train'][dataset_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_data['sentence'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model_2.bert.embeddings.word_embeddings(torch.tensor(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bert_model_2.bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(line_text_encoding, \n",
    "                    add_special_tokens=True, \n",
    "                    padding= 'max_length',  # 'max_length'\n",
    "                    max_length= 128,  # args.max_seq_length\n",
    "                    truncation=True,\n",
    "                    return_attention_mask=True, \n",
    "                    return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs1 = model1.roberta.embeddings.word_embeddings(inputs['input_ids'])\n",
    "    outputs2 = model2.roberta.embeddings.word_embeddings(inputs['input_ids'])\n",
    "print(outputs1)\n",
    "print()\n",
    "print(outputs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_data = datasets['mnli']['train'][:20]\n",
    "input_token = []\n",
    "outputs = []\n",
    "inputs = tokenizer(sentences_data, \n",
    "                    add_special_tokens=True, \n",
    "                    padding= 'max_length',  # 'max_length'\n",
    "                    max_length= 128,  # args.max_seq_length\n",
    "                    truncation=True,\n",
    "                    return_attention_mask=True, \n",
    "                    return_tensors='pt')\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(len(sentences_data)):\n",
    "        temp = model.roberta.embeddings.word_embeddings(inputs['input_ids'][i])\n",
    "        input_token.append(inputs['input_ids'][0])\n",
    "        outputs.append(temp)\n",
    "len(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_map = {0: 'neutral', 1: 'entailment', 2: 'contradiction'}\n",
    "aa = '/workspace/test_result'\n",
    "aaa = [1,1,1,1,1,1,1]\n",
    "\n",
    "with open(os.path.join(aa, 'test.tsv'), \"w\") as f:\n",
    "    f.write('index\\tprediction\\n')\n",
    "    for i, pred in enumerate(aaa):\n",
    "        f.write(f'{i}\\t{pred_map[pred]}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/workspace/task_dataset/sst-2_dataset.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sst2_train = sst2['train']\n",
    "sst2_train_sentence = sst2_train['sentence']\n",
    "sst2_train_sentence[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpretation Embedding Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
    "\n",
    "with open('/workspace/model_file/roberta-large_class_2.pkl', 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    "with open('/workspace/model_file/roberta-large_tokenizer.pkl', 'rb') as f:\n",
    "    tokenizer = pickle.load(f)\n",
    "\n",
    "with open('/workspace/cache_VQ_model_ver2/roberta-large_5e-05_4096.pkl', 'rb') as f:\n",
    "    vq_model = pickle.load(f).to('cuda')    \n",
    "\n",
    "with open('/workspace/dataset/roberta_large_embeddings.pkl', 'rb') as f:\n",
    "    embeddings = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_embedding_to_token(embeddings, word_embedding):\n",
    "#     embeddings = embeddings.detach()\n",
    "#     word_embedding = word_embedding.detach()\n",
    "\n",
    "#     similarity = cosine_similarity(embeddings, word_embedding)\n",
    "#     return torch.argmax(torch.tensor(similarity)).item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = [13,42,100]\n",
    "sst2_index = {}\n",
    "for seed in seeds:\n",
    "    with open(f'/workspace/cache_index/sst-2_seed_{seed}_indexes.pkl', 'rb') as f:\n",
    "        sst2_index[f'seed_{seed}'] = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sst2_index['seed_13'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sst2_index['seed_42'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sst2_index['seed_100'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, token in zip(token_indices, specific_tokens):\n",
    "    plt.annotate(token, (reduced_embeddings[index, 0], reduced_embeddings[index, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "def sum_text(texts1, texts2, sep_token):\n",
    "    full_text = np.array(texts1,dtype=object) + np.array([sep_token for _ in range(len(texts1))])+np.array(texts2,dtype=object)\n",
    "\n",
    "    return list(full_text)\n",
    "\n",
    "datasets = {}\n",
    "data_names = ['qqp', 'cola','sst-2', 'mrpc', 'wnli', 'qnli', 'rte', 'mnli', 'mnli-mm']\n",
    "for data_name in data_names:\n",
    "    location = f'/workspace/task_dataset/{data_name}_dataset.pkl'\n",
    "    with open(location, 'rb') as f:\n",
    "        datasets[data_name] = pickle.load(f)\n",
    "\n",
    "# # mnli dataset\n",
    "# sentences_data = datasets['mnli']['train'][:20]\n",
    "# line_text_encoding = sum_text(sentences_data['premise'],sentences_data['hypothesis'],tokenizer.sep_token)\n",
    "\n",
    "\n",
    "sentences_data = datasets['sst-2']['train'][sst2_index['seed_42']]\n",
    "line_text_encoding = sentences_data['sentence']\n",
    "\n",
    "\n",
    "# # Set the datasets\n",
    "embeddings = embeddings.detach()\n",
    "\n",
    "input_token = []\n",
    "outputs = []\n",
    "inputs = tokenizer(line_text_encoding, \n",
    "                    add_special_tokens=True, \n",
    "                    padding= 'max_length',  # 'max_length'\n",
    "                    max_length= 128,  # args.max_seq_length\n",
    "                    truncation=True,\n",
    "                    return_attention_mask=True, \n",
    "                    return_tensors='pt')\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(len(line_text_encoding)):\n",
    "        temp = model.roberta.embeddings.word_embeddings(inputs['input_ids'][i])\n",
    "        input_token.append(inputs['input_ids'][0])\n",
    "        outputs.append(temp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets['mnli']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert embedding to token\n",
    "\n",
    "def get_embedding_to_token_cosine(embeddings, outputs, sentences_data):\n",
    "    for i in range(len(outputs)):\n",
    "        word_embedding = outputs[i].detach()\n",
    "        print(sentences_data[i])\n",
    "        similarity = cosine_similarity(embeddings, word_embedding)\n",
    "        for i in range(similarity.shape[1]):\n",
    "            print(tokenizer.decode(similarity[:,i].argmax()), end = ' ')\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert embedding to token\n",
    "\n",
    "def get_embedding_to_token_euclidean(embeddings, outputs, sentences_data):\n",
    "    for i in range(len(outputs)):\n",
    "        word_embedding = outputs[i].detach()\n",
    "        print(sentences_data[i])\n",
    "        similarity = euclidean_distances(embeddings, word_embedding)\n",
    "        for i in range(similarity.shape[1]):\n",
    "            print(tokenizer.decode(similarity[:,i].argmin()), end = ' ')\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# real sentence 분석\n",
    "get_embedding_to_token_euclidean(embeddings, outputs, line_text_encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# real sentence 분석\n",
    "get_embedding_to_token_cosine(embeddings, outputs, line_text_encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vq_input = outputs.copy()\n",
    "print(len(vq_input))\n",
    "print(vq_input[0].shape)\n",
    "# vq_input_ = [torch.tensor(i) for i in vq_input]\n",
    "\n",
    "\n",
    "augmented_data = []\n",
    "total_augmented_data = []\n",
    "\n",
    "\n",
    "vq_model.eval()\n",
    "for embs in vq_input:\n",
    "    embs = embs.to(device = 'cuda:0')\n",
    "    decoded_sentence = []\n",
    "    for j in range(embs.shape[0]):\n",
    "        text = embs[j,:].view(1,-1)\n",
    "        # label = label.view(-1)\n",
    "        decoded_text = vq_model(text)[0]\n",
    "        decoded_sentence.append(decoded_text)\n",
    "    augmented_data = torch.cat(decoded_sentence, dim=0)\n",
    "    total_augmented_data.append(augmented_data)\n",
    "total_augmented_data = torch.stack(total_augmented_data, dim=0)\n",
    "attention_mask = inputs['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_augmented_data = total_augmented_data.to(device = 'cpu')\n",
    "total_augmented_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fake data 분석\n",
    "get_embedding_to_token_euclidean(embeddings, total_augmented_data, line_text_encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fake data 분석\n",
    "get_embedding_to_token_cosine(embeddings, total_augmented_data, line_text_encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert embedding to token\n",
    "\n",
    "def get_embedding_to_token_cosine_vq(embeddings, outputs, vq_outputs, sentences_data):\n",
    "    rate = 0.25\n",
    "    for i in range(len(outputs)):\n",
    "        word_embedding_real = outputs[i].detach()\n",
    "        word_embedding_fake = vq_outputs[i].detach()\n",
    "        print(sentences_data[i])\n",
    "        similarity_real = cosine_similarity(embeddings, word_embedding_real)\n",
    "        similarity_fake = cosine_similarity(embeddings, word_embedding_fake*(1-rate) + word_embedding_real * rate)\n",
    "        for j in range(similarity_real.shape[1]):\n",
    "            print('real[',tokenizer.decode(similarity_real[:,j].argmax()) , '] fake[', tokenizer.decode(similarity_fake[:,j].argmax()),']')\n",
    "            print()\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert embedding to token\n",
    "\n",
    "def get_embedding_to_token_euclidean_vq(embeddings, outputs, vq_outputs, sentences_data):\n",
    "    rate = 0.3\n",
    "    for i in range(len(outputs)):\n",
    "        word_embedding_real = outputs[i].detach()\n",
    "        word_embedding_fake = vq_outputs[i].detach()\n",
    "        print(sentences_data[i])\n",
    "        similarity_real = euclidean_distances(embeddings, word_embedding_real)\n",
    "        similarity_fake = euclidean_distances(embeddings, word_embedding_fake*(1-rate) + word_embedding_real * rate)\n",
    "        for j in range(similarity_real.shape[1]):\n",
    "            print('real[',tokenizer.decode(similarity_real[:,j].argmin()) , '] fake[', tokenizer.decode(similarity_fake[:,j].argmin()),']')\n",
    "            print()\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rate = 0.0\n",
    "for i in range(len(outputs)):\n",
    "    word_embedding_real = outputs[i].detach()\n",
    "    word_embedding_fake = total_augmented_data[i].detach()\n",
    "    print(line_text_encoding[i])\n",
    "    similarity_real = cosine_similarity(embeddings, word_embedding_real)\n",
    "    similarity_fake = cosine_similarity(embeddings, word_embedding_fake*(1-rate) + word_embedding_real * rate)\n",
    "    for j in range(similarity_real.shape[1]):\n",
    "        print('real[',tokenizer.decode(similarity_real[:,j].argmax()) , ']' )\n",
    "        print('real similar top 5 [', end = ' ')\n",
    "        for k in range(5):\n",
    "            print(tokenizer.decode(similarity_real[:,j].argsort()[-5:][-(k+1)]), end = ' ')\n",
    "        print(']')\n",
    "\n",
    "        print('fake similar top 5 [', end = ' ')\n",
    "        for k in range(5):\n",
    "            print(tokenizer.decode(similarity_fake[:,j].argsort()[-5:][-(k+1)]), end = ' ')\n",
    "        print(']')\n",
    "\n",
    "        print()\n",
    "    print('\\n')\n",
    "# cosine_similarity(embeddings, outputs[10:15]*(1-rate) + total_augmented_data[10:15] * rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rate = 0.2\n",
    "for i in range(len(outputs)):\n",
    "    word_embedding_real = outputs[i].detach()\n",
    "    word_embedding_fake = total_augmented_data[i].detach()\n",
    "    print(line_text_encoding[i])\n",
    "    similarity_real = cosine_similarity(embeddings, word_embedding_real)\n",
    "    similarity_fake = cosine_similarity(embeddings, word_embedding_fake*(1-rate) + word_embedding_real * rate)\n",
    "    for j in range(similarity_real.shape[1]):\n",
    "        print('real[',tokenizer.decode(similarity_real[:,j].argmax()) , ']' )\n",
    "        print('real similar top 5 [', end = ' ')\n",
    "        for k in range(5):\n",
    "            print(tokenizer.decode(similarity_real[:,j].argsort()[-5:][-(k+1)]), end = ' ')\n",
    "        print(']')\n",
    "\n",
    "        print('fake similar top 5 [', end = ' ')\n",
    "        for k in range(5):\n",
    "            print(tokenizer.decode(similarity_fake[:,j].argsort()[-5:][-(k+1)]), end = ' ')\n",
    "        print(']')\n",
    "\n",
    "        print()\n",
    "    print('\\n')\n",
    "# cosine_similarity(embeddings, outputs[10:15]*(1-rate) + total_augmented_data[10:15] * rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rate = 0.4\n",
    "for i in range(len(outputs)):\n",
    "    word_embedding_real = outputs[i].detach()\n",
    "    word_embedding_fake = total_augmented_data[i].detach()\n",
    "    print(line_text_encoding[i])\n",
    "    similarity_real = cosine_similarity(embeddings, word_embedding_real)\n",
    "    similarity_fake = cosine_similarity(embeddings, word_embedding_fake*(1-rate) + word_embedding_real * rate)\n",
    "    for j in range(similarity_real.shape[1]):\n",
    "        print('real[',tokenizer.decode(similarity_real[:,j].argmax()) , ']' )\n",
    "        print('real similar top 5 [', end = ' ')\n",
    "        for k in range(5):\n",
    "            print(tokenizer.decode(similarity_real[:,j].argsort()[-5:][-(k+1)]), end = ' ')\n",
    "        print(']')\n",
    "\n",
    "        print('fake similar top 5 [', end = ' ')\n",
    "        for k in range(5):\n",
    "            print(tokenizer.decode(similarity_fake[:,j].argsort()[-5:][-(k+1)]), end = ' ')\n",
    "        print(']')\n",
    "\n",
    "        print()\n",
    "    print('\\n')\n",
    "# cosine_similarity(embeddings, outputs[10:15]*(1-rate) + total_augmented_data[10:15] * rate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cosine Similarity로 Convert 하는 방법\n",
    "\n",
    "## Random Gaussian distribution을 기준으로 만들어진 Sample과 원래 데이터를 섞었을 때의 변화\n",
    "\n",
    "random_data = []\n",
    "\n",
    "for _ in range(len(outputs)):\n",
    "    random_data.append(torch.tensor(np.random.normal(0, 1, (128, 1024))))\n",
    "\n",
    "random_data = torch.stack(random_data, dim = 0).to(device = 'cpu')\n",
    "\n",
    "for rate in [0.8, 0.4, 0.2]:\n",
    "    print('rate : ', rate)\n",
    "    for i in range(len(outputs)):\n",
    "        word_embedding_real = outputs[i].detach()\n",
    "        word_embedding_fake = random_data[i].detach()\n",
    "        print(line_text_encoding[i])\n",
    "        similarity_real = cosine_similarity(embeddings, word_embedding_real)\n",
    "        similarity_fake = cosine_similarity(embeddings, word_embedding_fake*(1-rate) + word_embedding_real * rate)\n",
    "        for i in range(similarity_real.shape[1]):\n",
    "            print('real[',tokenizer.decode(similarity_real[:,i].argmax()) , ']' )\n",
    "            print('real similar top 5 [', end = ' ')\n",
    "            for j in range(5):\n",
    "                print(tokenizer.decode(similarity_real[:,i].argsort()[-5:][-(j+1)]), end = ' ')\n",
    "            print(']')\n",
    "\n",
    "            print('fake similar top 5 [', end = ' ')\n",
    "            for j in range(5):\n",
    "                print(tokenizer.decode(similarity_fake[:,i].argsort()[-5:][-(j+1)]), end = ' ')\n",
    "            print(']')\n",
    "\n",
    "            print()\n",
    "        print('\\n')\n",
    "    # cosine_similarity(embeddings, outputs[10:15]*(1-rate) + random_data[10:15] * rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Euclidian distance로 Convert하는 방법\n",
    "\n",
    "rate = 0.0\n",
    "for i in range(len(outputs)):\n",
    "    word_embedding_real = outputs[i].detach()\n",
    "    word_embedding_fake = total_augmented_data[i].detach()\n",
    "    print(line_text_encoding[i])\n",
    "    similarity_real = euclidean_distances(embeddings, word_embedding_real)\n",
    "    similarity_fake = euclidean_distances(embeddings, word_embedding_fake*(1-rate) + word_embedding_real * rate)\n",
    "    for i in range(similarity_real.shape[1]):\n",
    "        print('real[',tokenizer.decode(similarity_real[:,i].argmin()) , ']' )\n",
    "        print('real similar top 5 [', end = ' ')\n",
    "        for j in range(5):\n",
    "            print(tokenizer.decode(similarity_real[:,i].argsort()[:5][j]), end = ' ')\n",
    "        print(']')\n",
    "\n",
    "        print('fake similar top 5 [', end = ' ')\n",
    "        for j in range(5):\n",
    "            print(tokenizer.decode(similarity_fake[:,i].argsort()[:5][j]), end = ' ')\n",
    "        print(']')\n",
    "\n",
    "        print()\n",
    "    print('\\n')\n",
    "# euclidean_distances(embeddings, outputs[10:15]*(1-rate) + total_augmented_data[10:15] * rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Euclidian distance로 Convert하는 방법\n",
    "\n",
    "rate = 0.4\n",
    "for i in range(len(outputs)):\n",
    "    word_embedding_real = outputs[i].detach()\n",
    "    word_embedding_fake = total_augmented_data[i].detach()\n",
    "    print(line_text_encoding[i])\n",
    "    similarity_real = euclidean_distances(embeddings, word_embedding_real)\n",
    "    similarity_fake = euclidean_distances(embeddings, word_embedding_fake*(1-rate) + word_embedding_real * rate)\n",
    "    for i in range(similarity_real.shape[1]):\n",
    "        print('real[',tokenizer.decode(similarity_real[:,i].argmin()) , ']' )\n",
    "        print('real similar top 5 [', end = ' ')\n",
    "        for j in range(5):\n",
    "            print(tokenizer.decode(similarity_real[:,i].argsort()[:5][j]), end = ' ')\n",
    "        print(']')\n",
    "\n",
    "        print('fake similar top 5 [', end = ' ')\n",
    "        for j in range(5):\n",
    "            print(tokenizer.decode(similarity_fake[:,i].argsort()[:5][j]), end = ' ')\n",
    "        print(']')\n",
    "\n",
    "        print()\n",
    "    print('\\n')\n",
    "# euclidean_distances(embeddings, outputs[10:15]*(1-rate) + total_augmented_data[10:15] * rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Euclidian distance로 Convert하는 방법\n",
    "\n",
    "rate = 0.8\n",
    "for i in range(len(outputs)):\n",
    "    word_embedding_real = outputs[i].detach()\n",
    "    word_embedding_fake = total_augmented_data[i].detach()\n",
    "    print(line_text_encoding[i])\n",
    "    similarity_real = euclidean_distances(embeddings, word_embedding_real)\n",
    "    similarity_fake = euclidean_distances(embeddings, word_embedding_fake*(1-rate) + word_embedding_real * rate)\n",
    "    for i in range(similarity_real.shape[1]):\n",
    "        print('real[',tokenizer.decode(similarity_real[:,i].argmin()) , ']' )\n",
    "        print('real similar top 5 [', end = ' ')\n",
    "        for j in range(5):\n",
    "            print(tokenizer.decode(similarity_real[:,i].argsort()[:5][j]), end = ' ')\n",
    "        print(']')\n",
    "\n",
    "        print('fake similar top 5 [', end = ' ')\n",
    "        for j in range(5):\n",
    "            print(tokenizer.decode(similarity_fake[:,i].argsort()[:5][j]), end = ' ')\n",
    "        print(']')\n",
    "\n",
    "        print()\n",
    "    print('\\n')\n",
    "# euclidean_distances(embeddings, outputs[10:15]*(1-rate) + total_augmented_data[10:15] * rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_embedding_to_token_cosine_vq(embeddings,outputs[10:15],total_augmented_data[10:15], line_text_encoding[10:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation Visualization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# TSNE_fit = TSNE(n_components=2).fit(embeddings)\n",
    "reduced_embeddings = TSNE(n_components=2).fit_transform(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1], alpha=0.1)\n",
    "plt.title('t-SNE visualization of RoBERTa embeddings')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(32):\n",
    "    print(i)\n",
    "    print(line_text_encoding[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# TSNE_fit = TSNE(n_components=2).fit(embeddings)\n",
    "reduced_embeddings = TSNE(n_components=2).fit_transform(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rate = 0.2\n",
    "for i in range(12,13):\n",
    "    word_embedding_real = outputs[i].detach()\n",
    "    word_embedding_fake = total_augmented_data[i].detach()\n",
    "    print(line_text_encoding[i])\n",
    "    similarity_real = cosine_similarity(embeddings, word_embedding_real)\n",
    "    similarity_fake = cosine_similarity(embeddings, word_embedding_fake*(1-rate) + word_embedding_real * rate)\n",
    "    for j in range(similarity_real.shape[1]):\n",
    "        print('real[',tokenizer.decode(similarity_real[:,j].argmax()) , ']' )\n",
    "        print('real similar top 5 [', end = ' ')\n",
    "        for k in range(5):\n",
    "            print(tokenizer.decode(similarity_real[:,j].argsort()[-5:][-(k+1)]), end = ' ')\n",
    "        print(']')\n",
    "        print('real similar index top 5 [', end = ' ')\n",
    "        for k in range(5):\n",
    "            print(similarity_real[:,j].argsort()[-5:][-(k+1)], end = ' ')\n",
    "        print(']')\n",
    "        print('fake similar top 5 [', end = ' ')\n",
    "        for k in range(5):\n",
    "            print(tokenizer.decode(similarity_fake[:,j].argsort()[-5:][-(k+1)]), end = ' ')\n",
    "        print(']')\n",
    "        print('fake similar index top 5 [', end = ' ')\n",
    "        for k in range(5):\n",
    "            print(similarity_fake[:,j].argsort()[-5:][-(k+1)], end = ' ')\n",
    "        print(']')\n",
    "        print()\n",
    "    print('\\n')\n",
    "# cosine_similarity(embeddings, outputs[10:15]*(1-rate) + total_augmented_data[10:15] * rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# character\n",
    "rate = 0.0\n",
    "rate_00 = outputs[12][9].detach()*(rate) + total_augmented_data[12][9].detach()*(1-rate)\n",
    "rate = 0.2\n",
    "rate_02 = outputs[12][9].detach()*(rate) + total_augmented_data[12][9].detach()*(1-rate)\n",
    "rate = 0.4\n",
    "rate_04 = outputs[12][9].detach()*(rate) + total_augmented_data[12][9].detach()*(1-rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = []\n",
    "embed.append(embeddings)\n",
    "embed.append(rate_00.reshape(1,-1))\n",
    "embed.append(rate_02.reshape(1,-1))\n",
    "embed.append(rate_04.reshape(1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specific_tokens = [\" bad\", \" nefarious\"]\n",
    "# token_indices = [tokenizer.encode(token)[1] for token in specific_tokens]\n",
    "\n",
    "# bad\n",
    "# token_indices_1 = [ 1099, 5654, 6587, 39485, 11385 ]\n",
    "# nefarious\n",
    "# token_indices_2 = [ 33952, 39701, 43282, 34056, 28368 ]\n",
    "\n",
    "# character\n",
    "token_indices_1 = [ 3768, 2048, 44217, 37813, 32254 ]\n",
    "# bad\n",
    "token_indices_2 = [31937, 32254, 3768, 27056, 37813 ]\n",
    "\n",
    "token_indices_3 = [50265,50266,50267,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_indices_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_indices_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot all embeddings in light gray\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1], alpha=0.5, linewidths=0.5, color='lightgray', label='All tokens')\n",
    "\n",
    "# Plot specific tokens with their respective colors\n",
    "real_token_colors = ['pink', 'gray']\n",
    "colors = ['green', 'red', 'blue', 'orange', 'purple']\n",
    "for index, color in zip(token_indices_1[:2], real_token_colors):\n",
    "    plt.scatter(reduced_embeddings[index, 0], reduced_embeddings[index, 1], alpha=1.0,   color=color, label=tokenizer.decode(index))  #\n",
    "for index, color in zip(token_indices_2, colors):\n",
    "    plt.scatter(reduced_embeddings[index, 0], reduced_embeddings[index, 1], alpha=1.0,  color=color, label=tokenizer.decode(index))  # \n",
    "\n",
    "# colors = ['black','black','black']\n",
    "# for index, color in zip(token_indices_3, colors):\n",
    "#     plt.scatter(reduced_embeddings[index, 0], reduced_embeddings[index, 1], alpha=0.8, color=color, label=[f'fake{index}'])\n",
    "\n",
    "plt.legend()\n",
    "plt.title('t-SNE visualization of RoBERTa embeddings with specific tokens highlighted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_embedding_to_token_euclidean_vq(embeddings,outputs[10:15],total_augmented_data[10:15], line_text_encoding[10:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    word_embedding = total_augmented_data[i].detach()\n",
    "    print(sentences_data[i])\n",
    "    similarity = cosine_similarity(embeddings, word_embedding)\n",
    "    for i in range(similarity.shape[1]):\n",
    "        print(tokenizer.decode(similarity[:,i].argmax()), end = ' ')    \n",
    "        # print(tokenizer.decode(similarity[:,i].argmax()), end = ' ')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device = 'cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vq_input__ = torch.stack(vq_input_, dim=0)\n",
    "vq_input__.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vq_input_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vq_input__ = vq_input__.to(device = 'cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vq_input__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(model.lm_head(vq_input__).shape[0]):\n",
    "    for i in range(model.lm_head(vq_input__).shape[1]):\n",
    "        predicted_index = torch.argmax(model.lm_head(vq_input__)[j,i,:]).item()\n",
    "        print(torch.argmax(model.lm_head(vq_input__)[j,i,:]).item())\n",
    "        predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])[0]\n",
    "        print(f\"Predicted token: {predicted_token}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vq_input__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vq_input__ = vq_input__.to(device = 'cpu')\n",
    "total_augmented_data = total_augmented_data.to(device = 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vq_input__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vq_input__[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_augmented_data[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(13):\n",
    "    print(cosine_similarity(vq_input__[0][i].detach().numpy(), total_augmented_data[0][i].detach().numpy().T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(13):\n",
    "    print(cosine_similarity(vq_input__[1][i].detach().numpy(), total_augmented_data[1][i].detach().numpy().T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_augmented_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(model.lm_head(total_augmented_data).shape[0]):\n",
    "    for i in range(model.lm_head(total_augmented_data).shape[1]):\n",
    "        predicted_index = torch.argmax(model.lm_head(total_augmented_data)[j,i,:]).item()\n",
    "        print(torch.argmax(model.lm_head(total_augmented_data)[j,i,:]).item())\n",
    "        predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])[0]\n",
    "\n",
    "        print(f\"Predicted token: {predicted_token}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the predicted token\n",
    "model.lm_head()\n",
    "\n",
    "predicted_index = torch.argmax(model.lm_head(total_augmented_data)[0,:,:]).item()\n",
    "predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])[0]\n",
    "\n",
    "print(f\"Predicted token: {predicted_token}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.argmax(predictions[0, 6]).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.convert_ids_to_tokens([predicted_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[0,6].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[0,6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs['input_ids'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs[\"input_ids\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.mask_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 모델 불러오기\n",
    "\n",
    "# from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "# import torch\n",
    "\n",
    "# # Load tokenizer and model\n",
    "# tokenizer = RobertaTokenizer.from_pretrained('roberta-large')\n",
    "# model = RobertaForSequenceClassification.from_pretrained('roberta-large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 모델 저장\n",
    "\n",
    "# import pickle\n",
    "\n",
    "# with open( 'model_file/roberta_large.pkl', 'wb') as f:\n",
    "#     pickle.dump(model, f)\n",
    "# with open( 'model_file/roberta_large_tokenizer.pkl', 'wb') as f:\n",
    "#     pickle.dump(tokenizer, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "# import pickle\n",
    "\n",
    "# text_dataset = {}\n",
    "\n",
    "# for data in ['sst2', 'cola', 'mrpc', 'qqp', 'stsb', 'mnli', 'qnli', 'rte', 'wnli', 'ax']:\n",
    "#     dataset = load_dataset(\"glue\", data)\n",
    "#     text_dataset[data] = dataset\n",
    "    \n",
    "    \n",
    "#     with open('dataset/%s_dataset.pkl'%data, 'wb') as f:\n",
    "#         pickle.dump(dataset, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file_path = os.getcwd() + '/model_file'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(model_file_path + '/roberta-large_class_2.pkl', \"rb\") as f:\n",
    "    model = pickle.load(f).to(device = 'cuda:1')\n",
    "with open(model_file_path + '/roberta-large_tokenizer.pkl', \"rb\") as f:\n",
    "    tokenizer = pickle.load(f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "dataset_names =  ['sst2', 'cola', 'mrpc', 'qqp', 'stsb', 'mnli', 'qnli', 'rte', 'wnli', 'ax']\n",
    "dataset = {}\n",
    "for dataset_name in dataset_names:\n",
    "    data_file_path = '%s/dataset/%s_dataset.pkl'%(os.getcwd(), dataset_name)\n",
    "    with open(data_file_path, \"rb\") as f:\n",
    "        temp_data = pickle.load(f)\n",
    "        dataset[dataset_name] = temp_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrpc = dataset['mrpc']\n",
    "qqp = dataset['qqp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qqp['test']['question1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "from random import randint\n",
    "\n",
    "def sum_text(texts1, texts2, self):\n",
    "    full_text = np.array(texts1,dtype=object) + np.array([self.tokenizer.sep_token for _ in range(len(texts1))])+np.array(texts2,dtype=object)\n",
    "\n",
    "    return list(full_text)\n",
    "\n",
    "\n",
    "class _Make_pair(Dataset):\n",
    "    def __init__(self, args, encodings, labels):\n",
    "        self.args = args\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # get the encoded input data for the given index\n",
    "        if self.args.data_loader_type == 'few-shot':\n",
    "            encoding = {key: tensor[index] for key, tensor in self.encodings.items()}\n",
    "\n",
    "        # get the corresponding label for the given index\n",
    "        label = torch.tensor(self.labels[index])\n",
    "\n",
    "        return encoding, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[dataset_names[5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_eval = 'train'\n",
    "dataset = mrpc[train_eval]\n",
    "dataset.num_rows\n",
    "num_classes = len(set(dataset['label']))\n",
    "num_classes\n",
    "num_shots = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[dataset.num_rows-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = set(dataset['label'])\n",
    "# dataset[train_eval].num_rows\n",
    "\n",
    "break_point = 0\n",
    "tensor_index = {}\n",
    "for class_ in classes:\n",
    "    tensor_index[class_] = []\n",
    "while True:\n",
    "    value = randint(0, dataset.num_rows - 1)\n",
    "    for class_ in classes:\n",
    "        if dataset[value]['label'] == class_:\n",
    "            if value not in tensor_index[class_]:\n",
    "                if len(tensor_index[class_]) < num_shots:\n",
    "                    tensor_index[class_].append(value)\n",
    "                    break_point += 1\n",
    "                else:\n",
    "                    pass\n",
    "    if break_point == num_classes * num_shots:\n",
    "        break\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "tensor_index_total = []\n",
    "for i in classes:\n",
    "    tensor_index_total += tensor_index[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tensor_index_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in set(dataset['label']):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.num_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set( mrpc[train_eval]['label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa = np.array(mrpc['train']['sentence1'],dtype=object) + np.array([tokenizer.sep_token for _ in range(len(mrpc['train']['sentence1']))])+np.array(mrpc['train']['sentence2'],dtype=object)\n",
    "\n",
    "\n",
    "linetext = list(aa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(linetext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer(linetext, \n",
    "        add_special_tokens=True, \n",
    "        padding= 'max_length',  # 'max_length'\n",
    "        max_length= 128,  # args.max_seq_length\n",
    "        truncation=True,\n",
    "        return_attention_mask=True, \n",
    "        return_tensors='pt') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrpc['train'][0]['sentence1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrpc['train'][0]['sentence2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = ['1','2','3']\n",
    "b = ['4','5','6']\n",
    "\n",
    "[x[0] + x[1] for x in zip(a, b)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import add\n",
    "map(add , mrpc['train']['sentence1'], mrpc['train']['sentence2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.sep_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.encode(full_sentence, add_special_tokens=False,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = [tokenizer.cls_token_id]\n",
    "attention_mask = [1]\n",
    "token_type_dis = [0]\n",
    "for sent_id, input_text in enumerate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import logging\n",
    "# import pandas as pd\n",
    "\n",
    "# logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "# def input_example_to_tuple(example): \n",
    "#     if example['sentence2'] is None:\n",
    "#         return [example[sentence1]]\n",
    "#     else:\n",
    "#         return [example[sentence1], example['sentence2']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrpc['train']['sentence2'] is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_example_to_tuple(mrpc['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(zip(mrpc['train']['sentence1'], [tokenizer.sep_token for _ in range(len(mrpc['train']['sentence1']))], mrpc['train']['sentence2']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "*cls**sent_0**mask*,*+sentl_1**sep+*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.sep_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.all_special_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = tokenizer(sst2['train'][0]['sentence'], \n",
    "                        add_special_tokens=True, \n",
    "                        padding= 'max_length',  # 'max_length'\n",
    "                        max_length= 128,  # args.max_seq_length\n",
    "                        truncation=True,\n",
    "                        return_attention_mask=True, \n",
    "                        return_tensors='pt')  # 왜인지 encoded_plus를 쓰면 버그뜸...\n",
    "# args.max_seq_length = encoding['input_ids'].shape[1]\n",
    "print(encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(8,10):\n",
    "i = 9\n",
    "print(i)\n",
    "print(dataset_names[i])\n",
    "print(dataset[dataset_names[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset_names[2])\n",
    "print(dataset[dataset_names[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = 2\n",
    "for i in range(100):\n",
    "    print(dataset[dataset_names[num]]['train'][i]['sentence1'],dataset[dataset_names[num]]['train'][i]['sentence2'])\n",
    "    print(dataset[dataset_names[num]]['train'][i]['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = 2\n",
    "for i in range(100):\n",
    "    print(dataset[dataset_names[num]]['train'][i]['sentence2'],dataset[dataset_names[num]]['train'][i]['label'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[dataset_names[9]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'roberta_large'\n",
    "dataset = 'colaaaaa'\n",
    "vq_lr = 0.0001\n",
    "num_codebook_vectors = 512\n",
    "seed = 42\n",
    "\n",
    "if not os.path.exists(f'{os.getcwd()}/testset_acc/{model_name}_{dataset}_acc.txt'):\n",
    "    with open(f'{os.getcwd()}/testset_acc/{model_name}_{dataset}_acc.txt', 'w') as f:\n",
    "        f.close()\n",
    "\n",
    "with open(f'{os.getcwd()}/testset_acc/{model_name}_{dataset}_acc.txt', 'a') as f:\n",
    "    f.write('vq_lr : %s, num_codebook_vectors : %s seed : %s'%(vq_lr, num_codebook_vectors, seed))\n",
    "    f.write('\\n')\n",
    "    f.write('acc : %s'%(0.5))\n",
    "    f.write('\\n')\n",
    "\n",
    "seed = 12\n",
    "\n",
    "with open(f'{os.getcwd()}/testset_acc/{model_name}_{dataset}_acc.txt', 'a') as f:\n",
    "    f.write('vq_lr : %s, num_codebook_vectors : %s seed : %s'%(vq_lr, num_codebook_vectors, seed))\n",
    "    f.write('\\n')\n",
    "    f.write('acc : %s'%(0.5))\n",
    "    f.write('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'roberta_large'\n",
    "dataset = 'cola'\n",
    "vq_lr = 0.0001\n",
    "num_codebook_vectors = 512\n",
    "seed = 12\n",
    "with open(f'{os.getcwd()}/testset_acc/{model_name}_{dataset}_acc.txt', 'a') as f:\n",
    "    f.write('vq_lr : %s, num_codebook_vectors : %s seed : %s'%(vq_lr, num_codebook_vectors, seed))\n",
    "    f.write('\\n')\n",
    "    f.write('acc : %s'%(0.5))\n",
    "    f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
